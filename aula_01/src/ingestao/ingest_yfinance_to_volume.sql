-- Databricks notebook source
-- Volume para zona raw (arquivos JSON)
CREATE VOLUME IF NOT EXISTS lakehouse.raw.raw_yfinance

-- COMMAND ----------

-- MAGIC %python
-- MAGIC %pip install yfinance
-- MAGIC

-- COMMAND ----------

-- MAGIC %python
-- MAGIC dbutils.library.restartPython()

-- COMMAND ----------

-- MAGIC %python
-- MAGIC import yfinance as yf
-- MAGIC import pandas as pd
-- MAGIC from datetime import datetime, UTC
-- MAGIC from pyspark.sql import functions as F
-- MAGIC from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
-- MAGIC
-- MAGIC # Caminho do Volume UC (Raw Zone)
-- MAGIC RAW_BASE_PATH = "/Volumes/lakehouse/raw/raw_yfinance/commodities/latest_prices"
-- MAGIC
-- MAGIC def get_commodities_df() -> pd.DataFrame:
-- MAGIC     """
-- MAGIC     Retorna as últimas cotações (1 minuto) de Ouro, Petróleo e Prata via Yahoo Finance.
-- MAGIC     """
-- MAGIC     symbols = ["GC=F", "CL=F", "SI=F"]  # Ouro, Petróleo, Prata
-- MAGIC     dfs = []
-- MAGIC
-- MAGIC     for sym in symbols:
-- MAGIC         # Baixa o último preço (1 minuto)
-- MAGIC         try:
-- MAGIC             ultimo_df = yf.Ticker(sym).history(period="1d", interval="1m")[["Close"]].tail(1)
-- MAGIC             if ultimo_df.empty:
-- MAGIC                 continue
-- MAGIC
-- MAGIC             ultimo_df = ultimo_df.rename(columns={"Close": "preco"})
-- MAGIC             ultimo_df["ativo"] = sym
-- MAGIC             ultimo_df["moeda"] = "USD"
-- MAGIC             ultimo_df["horario_coleta"] = datetime.now(UTC)
-- MAGIC
-- MAGIC             dfs.append(ultimo_df[["ativo", "preco", "moeda", "horario_coleta"]])
-- MAGIC         except Exception as e:
-- MAGIC             print(f"⚠️ Erro ao buscar {sym}: {e}")
-- MAGIC
-- MAGIC     if not dfs:
-- MAGIC         raise ValueError("Nenhuma cotação retornada pelo Yahoo Finance.")
-- MAGIC
-- MAGIC     return pd.concat(dfs, ignore_index=True)
-- MAGIC
-- MAGIC
-- MAGIC # Coleta
-- MAGIC pdf = get_commodities_df()
-- MAGIC
-- MAGIC # Define schema explícito (boa prática para ingestion)
-- MAGIC schema = StructType([
-- MAGIC     StructField("ativo", StringType(), False),
-- MAGIC     StructField("preco", DoubleType(), False),
-- MAGIC     StructField("moeda", StringType(), False),
-- MAGIC     StructField("horario_coleta", TimestampType(), False),
-- MAGIC ])
-- MAGIC
-- MAGIC # Cria DataFrame Spark
-- MAGIC df = (
-- MAGIC     spark.createDataFrame(pdf, schema=schema)
-- MAGIC         .withColumn("ingestion_ts_utc", F.current_timestamp())
-- MAGIC         .withColumn("source_system", F.lit("yfinance"))
-- MAGIC         .withColumn("source_endpoint", F.lit("https://finance.yahoo.com"))
-- MAGIC         .withColumn("ingestion_date", F.to_date(F.col("ingestion_ts_utc")))
-- MAGIC )
-- MAGIC
-- MAGIC # Escrita no Volume UC, particionada por ingestion_date
-- MAGIC (
-- MAGIC     df.write
-- MAGIC       .mode("append")
-- MAGIC       .partitionBy("ingestion_date")
-- MAGIC       .json(RAW_BASE_PATH)
-- MAGIC )
-- MAGIC
-- MAGIC print("✅ JSON salvo em:", RAW_BASE_PATH)
-- MAGIC